# MIRAGE: Multi-Image Retrieval Augmented GEneralization

[![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/)  [![arXiv](https://img.shields.io/badge/arXiv-2407.13766-red)](https://arxiv.org/abs/2407.13766) 

***International Conference on Learning Representations (ICLR) 2025***

Welcome to the official repository for our paper: [Visual Haystacks: A Vision-Centric Needle-In-A-Haystack Benchmark](https://arxiv.org/abs/2407.13766). Explore our project page [here](https://visual-haystacks.github.io/) and the benchmark toolkits [here](https://github.com/visual-haystacks/vhs_benchmark)!

**Authors**: [Tsung-Han Wu](https://tsunghan-wu.github.io/), [Giscard Biamby](https://scholar.google.com/citations?user=s0Fof5IAAAAJ&hl=en), [Jerome Quenum](https://people.eecs.berkeley.edu/~jquenum/), [Ritwik Gupta](https://ritwikgupta.me/), [Joseph E. Gonzalez](https://people.eecs.berkeley.edu/~jegonzal/), [Trevor Darrell](https://people.eecs.berkeley.edu/~trevor/), [David M. Chan](https://dchan.cc/) at UC Berkeley. 

**Visual Haystacks (VHs) Benchmark Dataset**: [ğŸ¤— tsunghanwu/visual_haystacks](https://huggingface.co/datasets/tsunghanwu/visual_haystacks), [ğŸ™ Github Repo](https://github.com/visual-haystacks/vhs_benchmark)

**Model Checkpoint**: [ğŸ¤—tsunghanwu/mirage-llama3.1-8.3B](https://huggingface.co/tsunghanwu/mirage-llama3.1-8.3B)

## :rocket: Introduction

This paper addresses the challenge of answering questions across tens of thousands of images. Through extensive [experiments](https://github.com/visual-haystacks/vhs_benchmark) through our Visual Haystacks (VHs) benchmark, we demonstrated that existing Large Multimodal Models (LMMs) struggle with inputs exceeding 100 images due to API limitations, context overflow, or hardware constraints on 4 A100 GPUs. These models often face issues such as visual distractions, cross-image reasoning difficulties, and positional biases. To overcome these challenges, we developed MIRAGE (8.3B), a pioneering, open-source visual-RAG baseline model based on LMMs capable of handling tens of thousands of images. In brief, MIRAGE integrates a compressor module that reduces image tokens by 18x, a dynamic query-aware retriever to filter irrelevant images, and a custom-trained LMM that can do multi-image reasoning. MIRAGE sets a new standard in open-source performance on the Visual Haystacks (VHs) benchmark and delivers solid results on both single- and multi-image question answering tasks.

![](assets/MIRAGE.png)

## :wrench: Installation Guide

1. Clone this repository and navigate to mirage folder
```bash
git clone https://github.com/visual-haystacks/mirage.git
cd mirage
```

2. Install Package
```Shell
conda create -n mirage python=3.10 -y
conda activate mirage
pip install --upgrade pip  # enable PEP 660 support
pip install -e .
```

3. Install additional packages for training cases
```
pip install -e ".[train]"
pip install flash-attn --no-build-isolation --no-cache-dir
```

## :gear: Quick Start / Demo
- Model Checkpoint: [ğŸ¤—tsunghanwu/mirage-llama3.1-8.3B](https://huggingface.co/tsunghanwu/mirage-llama3.1-8.3B)
- Demo code (single test case): `CUDA_VISIBLE_DEVICES=X python3 demo.py --model-path [huggingface model id or local path] --image-folder [local image folder] --prompt [prompt path]`
- Hereâ€™s a sample output from MIRAGE using some photos I took on my iPhone. (Feel free to give it a star if you think my cat is adorable! ğŸ˜ºâœ¨)
![](assets/demo.png)

<h2>ğŸ“ˆ Evaluation</h2>

<h3>1. Data Preparation</h3>
<ul>
    <li>For Visual Haystacks (VHs), download the data from <a href="https://huggingface.co/datasets/tsunghanwu/visual_haystacks">ğŸ¤— tsunghanwu/visual_haystacks</a> and place it in <code>playground/data/eval/visual_haystacks</code>.</li>
    <li>For single-image QA, download all data according to <a href="https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md">LLaVA's instructions</a>.</li>
    <li>For multi-image QA, download <a href="https://drive.google.com/file/d/1_YHHNGJqprT30XPUyVW9F8oGOxuavVwN/view?usp=sharing">RETVQA's test set</a> and place it in <code>playground/data/eval/retvqa</code>. For evaluation, refer to <a href="https://github.com/Abhiram4572/mi_bart">RETVQA's GitHub Repo</a>.</li>
</ul>
<p>In summary, the data structure of <code>playground/data/eval</code> should look like this:</p>
<details>
    <summary>Show Data Structure</summary>
    <pre>
playground/data/eval/
â”œâ”€â”€ gqa
â”‚   â”œâ”€â”€ answers
â”‚   â”œâ”€â”€ data                   # directory
â”‚   â”œâ”€â”€ llava_gqa_testdev_balanced.jsonl
â”œâ”€â”€ mmbench
â”‚   â”œâ”€â”€ answers
â”‚   â”œâ”€â”€ answers_upload
â”‚   â””â”€â”€ mmbench_dev_20230712.tsv
â”œâ”€â”€ mmbench_cn
â”‚   â”œâ”€â”€ answers
â”‚   â”œâ”€â”€ answers_upload
â”‚   â””â”€â”€ mmbench_dev_cn_20231003.tsv
â”œâ”€â”€ mm-vet
â”‚   â”œâ”€â”€ answers
â”‚   â”œâ”€â”€ images                  # directory
â”‚   â”œâ”€â”€ llava-mm-vet.jsonl
â”‚   â””â”€â”€ results
â”œâ”€â”€ pope
â”‚   â”œâ”€â”€ answers
â”‚   â”œâ”€â”€ coco                    # directory (point to COCO2014)
â”‚   â””â”€â”€ llava_pope_test.jsonl
â”œâ”€â”€ retvqa
â”‚   â”œâ”€â”€ answers
â”‚   â”œâ”€â”€ vg                     # directory (point to Visual Genome directory)
â”‚   â””â”€â”€ retvqa_test_mirage.json
â”œâ”€â”€ textvqa
â”‚   â”œâ”€â”€ answers
â”‚   â”œâ”€â”€ llava_textvqa_val_v051_ocr.jsonl
â”‚   â”œâ”€â”€ TextVQA_0.5.1_val.json
â”‚   â””â”€â”€ train_images           # directory (download from their website)
â”œâ”€â”€ visual_haystacks
â”‚   â”œâ”€â”€ coco             # directory (point to COCO2017)
â”‚   â””â”€â”€ VHs_qa           # directory (download from VHs' huggingface)
â”œâ”€â”€ vizwiz
â”‚   â”œâ”€â”€ answers
â”‚   â”œâ”€â”€ answers_upload
â”‚   â”œâ”€â”€ llava_test.jsonl
â”‚   â””â”€â”€ test                   # directory (download from their website)
â””â”€â”€ vqav2
    â”œâ”€â”€ answers
    â”œâ”€â”€ answers_upload
    â”œâ”€â”€ llava_vqav2_mscoco_test2015.jsonl
    â”œâ”€â”€ llava_vqav2_mscoco_test-dev2015.jsonl
    â””â”€â”€ test2015               # directory (download from their website)
    </pre>
</details>

<h3>2. Run Scripts</h3>
<pre><code># Visual Haystacks
CUDA_VISIBLE_DEVICES=0 bash scripts/eval/{vhs_single,vhs_multi}.sh
# VQAv2, GQA, RetVQA
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 bash scripts/eval/{vqav2,gqa,retvqa}.sh
# Vizwiz, TextVQA, POPE, MMBench, MMBench-CN, MM-Vet
CUDA_VISIBLE_DEVICES=0 bash scripts/eval/{vizwiz,textvqa,pope,mmbench,mmbench_cn,mmvet}.sh
</code></pre>

<h3>3. Results</h3>
<p><img src="./assets/result_vhs.png" alt="Results of Visual Haystacks" /></p>
<table>
    <thead>
        <tr>
            <th>Checkpoint</th>
            <th>VQAv2</th>
            <th>GQA</th>
            <th>VizWiz</th>
            <th>TextVQA</th>
            <th>POPE</th>
            <th>MM-Bench</th>
            <th>MM-Bench-CN</th>
            <th>MM-Vet</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><a href="https://huggingface.co/tsunghanwu/mirage-llama3.1-8.3B">ğŸ¤— tsunghanwu/mirage-llama3.1-8.3B</a></td>
            <td>76.56</td>
            <td>59.13</td>
            <td>40.52</td>
            <td>56.24</td>
            <td>85.40</td>
            <td>69.24</td>
            <td>66.92</td>
            <td>33.4</td>
        </tr>
    </tbody>
</table>


## :fire: Training

### 1. Data Preparation
  - Please download the dataset from [ğŸ¤— tsunghanwu/MIRAGE-training-set](https://huggingface.co/datasets/tsunghanwu/MIRAGE-training-set).
  - For stage-1 pre-training (training Q-Former and MLP projector), download datasets such as CC-12M, LAION-400M, and COCO.
  - For stages 2 and 3 pre-training, which involve training Q-Former/MLP projector with high-quality captions and training the downstream retriever module with augmented LLaVA data, download SAM, VG, COCO, TextVQA, OCR_VQA, and GQA to `playground/data`.
  - For instruction tuning, download SAM, VG, COCO, TextVQA, OCR_VQA, GQA, slidevqa, and webqa to `playground/data`.

Below is the expected data structure for `playground/data/eval`:

<details>
    <summary>Show Data Structure</summary>
    <pre>
playground/data/
â”œâ”€â”€ coco
â”‚   â”œâ”€â”€ annotations
â”‚   â”œâ”€â”€ test2017
â”‚   â”œâ”€â”€ train2017
â”‚   â””â”€â”€ val2017
â”œâ”€â”€ gqa
â”‚   â””â”€â”€ images
â”œâ”€â”€ ocr_vqa
â”‚   â””â”€â”€ images
â”œâ”€â”€ sam
â”‚   â””â”€â”€ images 
â”œâ”€â”€ share_textvqa
â”‚   â””â”€â”€ images
â”œâ”€â”€ slidevqa
â”‚   â””â”€â”€ images (download from https://drive.google.com/file/d/11bsX48cPpzCfPBnYJgSesvT7rWc84LpH/view)
â”œâ”€â”€ textvqa
â”‚   â””â”€â”€ train_images
â”œâ”€â”€ vg
â”‚   â”œâ”€â”€ VG_100K
â”‚   â””â”€â”€ VG_100K_2
â””â”€â”€ webqa
    â””â”€â”€ webqa_images (download from https://drive.google.com/drive/folders/1ApfD-RzvJ79b-sLeBx1OaiPNUYauZdAZ and convert them to .jpg format)
    </pre>
</details>

### 2. Pretraining/Finetuning

Run the following script with minor modifications as needed. Note: During the finetuning, we found that freezing the downstream retriever but only updating Q-Former/LLM leads to better performance on LLama-3.1-8b, whereas unfreezing the retriever yields better results on vicuna-v1.5-7b.

```bash
# Stage 1-3 Pretraining
bash scripts/pretrain_stage{1,2,3}.sh
# Instruction Finetuning
bash scripts/finetune_qformer_lora.sh
```

### 3. Weight Merging

Please merge LoRA weights back to the original checkpoint using the following code:

```bash
python scripts/merge_lora_weights.py \
    --model-path checkpoints/mirage_qformer_ft \
    --model-base meta-llama/Meta-Llama-3.1-8B-Instruct \
    --save-model-path your_output_path
```

## :pray: Acknowledgements

We are grateful for the foundational code provided by [LLaVA](https://github.com/haotian-liu/LLaVA) and [LLaVA-More](https://github.com/aimagelab/LLaVA-MORE). Utilizing their resources implies agreement with their respective licenses. Our project benefits greatly from these contributions, and we acknowledge their significant impact on our work. 

## :dart: Citation

If you use our work or our implementation in this repo or find them helpful, please consider giving a citation.
```
@article{wu2024visual,
  title={Visual Haystacks: A Vision-Centric Needle-In-A-Haystack Benchmark},
  author={Wu, Tsung-Han and Biamby, Giscard and and Quenum, Jerome and Gupta, Ritwik and Gonzalez, Joseph E and Darrell, Trevor and Chan, David M},
  journal={International Conference on Learning Representations},
  year={2025},
  url={https://arxiv.org/abs/2407.13766}
}
```
